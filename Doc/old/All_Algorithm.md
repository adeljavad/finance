

# لیست و تحلیل الگوریتم‌های مهم و کاربردی حسابرسی با قابلیت پیاده‌سازی ایجنتیک

## 1. الگوریتم‌های حسابرسی مبتنی بر حمله و استخراج مدل (Attack & Model Extraction-Based Auditing Algorithms)

### 1.1. الگوریتم‌های استخراج مدل (Model Extraction Attacks)

#### 1.1.1. الگوریتم Black-Box Ripper

الگوریتم **Black-Box Ripper** یک روش پیشرفته برای استخراج مدل‌های یادگیری ماشین است که در محیط‌های **جعبه سیاه (Black-Box)** عمل می‌کند. این الگوریتم که در کنفرانس **NeurIPS 2020** معرفی شد، برای مواردی طراحی شده است که هیچ اطلاعاتی در مورد ساختار مدل هدف یا داده‌های آموزش آن در دسترس نیست و تنها امکان دسترسی به API مدل وجود دارد . این الگوریتم با استفاده از **الگوریتم‌های تکاملی (Evolutionary Algorithms)** و مدل‌های تولیدی (Generative Models) مانند **VAE یا GAN**، اقدام به تولید داده‌های آموزشی مصنوعی می‌کند که رفتار مدل هدف را تقلید می‌نماید. در ادامه، این داده‌های تولید شده برای آموزش یک مدل جایگزین (Student Model) استفاده می‌شود که عملکردی مشابه مدل اصلی (Teacher Model) دارد. این روش به عنوان یک **حمله استخراج (Extraction Attack)** شناخته می‌شود که هدف آن کپی‌برداری از مدل است، نه خرابکاری در آن (Poisoning Attack) یا فریب دادن آن (Evasion Attack) .

فرآیند کلی Black-Box Ripper شامل چند مرحله کلیدی است. ابتدا، یک مدل تولیدی (Generator) آموزش داده می‌شود تا با استفاده از بردارهای فضای پنهان (Latent Space Vectors)، داده‌هایی در دامنه مورد نظر تولید کند. سپس، یک الگوریتم جستجوی تکاملی برای یافتن بردارهای فضای پنهان استفاده می‌شود که بتوانند تصاویری تولید کنند که مدل هدف را به درستی طبقه‌بندی می‌کند. این بردارهای بهینه شده سپس به مدل تولیدی داده می‌شوند تا یک مجموعه داده آموزشی مصنوعی (Proxy Dataset) ایجاد شود. در نهایت، این مجموعه داده برای آموزش مدل Student استفاده می‌شود. نتایج تجربی نشان داده‌اند که این روش می‌تواند مدل‌هایی با دقت بالا (در حدود **70٪**) تولید کند، حتی زمانی که دقت مدل هدف در حدود **80٪** است. این عملکرد از روش‌های دیگر مانند **Knockoff Net و Zero-Shot Knowledge Distillation (ZSKD)** بهتر است، اگرچه در برخی موارد ممکن است از DeGAN ضعیف‌تر عمل کند . این الگوریتم برای حسابرسی مدل‌ها بسیار مفید است، زیرا امکان تحلیل و بررسی یک مدل جعبه سیاه را بدون نیاز به دسترسی مستقیم به آن فراهم می‌کند.

#### 1.1.2. الگوریتم MeaeQ (Model extraction attack with efficient Queries)

الگوریتم **MeaeQ** یک روش کارآمد برای حملات استخراج مدل است که با هدف **کاهش تعداد کوئری‌ها** و افزایش کیفیت داده‌های استخراج‌شده طراحی شده است . این الگوریتم در سناریوهایی کاربرد دارد که دسترسی به مدل از طریق API محدود است و هزینه هر کوئری بالا است. MeaeQ از دو ماژول اصلی تشکیل شده است: **فیلتر ارتباط با وظیفه (Task Relevance Filter - TRF)** و **کاهش داده مبتنی بر خوشه‌بندی (Data Reduction based on Clustering - DRC)** . ماژول TRF با استفاده از یک طبقه‌بندکننده استنتاج توالی صفر-شات (zero-shot sequence inference classifier)، داده‌های مرتبط با وظیفه را از یک مجموعه داده متن باز عمومی جدا می‌کند. این کار باعث می‌شود که داده‌های استفاده‌شده برای کوئری زدن به API، از نظر معنایی به وظیفه مدل هدف نزدیک باشند. ماژول DRC نیز با استفاده از تکنیک‌های خوشه‌بندی روی بردارهای جاسازی‌شده (embedding vectors) متن، از شرایط نامتوازن کلاس‌ها (class imbalance) جلوگیری می‌کند و اطلاعات تکراری را حذف می‌کند. نتایج آزمایش‌ها نشان می‌دهد که MeaeQ می‌تواند با تعداد کوئری‌های کمتر، شباهت عملکردی بالاتری بین مدل استخراج‌شده و مدل هدف ایجاد کند . این الگوریتم به دلیل کارآمدی در استفاده از منابع و توانایی مقابله با چالش‌های دنیای واقعی مانند عدم توازن کلاس‌ها، یک ابزار مهم برای حسابرسی مدل‌های پیچیده در محیط‌های Black-Box است.

#### 1.1.3. الگوریتم SEEK (Secure Inference Attack)

الگوریتم **SEEK** یک حمله استخراج مدل است که علیه **پروتکل‌های استنتاج امن ترکیبی (Hybrid Secure Inference Protocols)** طراحی شده است . این پروتکل‌ها معمولاً از **رمزنگاری همومورفیک (HE)** برای لایه‌های خطی و **محاسبات چندجانبه (MPC)** برای لایه‌های غیرخطی استفاده می‌کنند تا حریم خصوصی مدل و داده‌های ورودی را حفظ کنند. حمله SEEK از آسیب‌پذیری در فرآیند تبدیل بین لایه‌های خطی و غیرخطی استفاده می‌کند. در این حمله، مهاجم با تغییر دادن ویژگی‌های میانی (intermediate features) و مشاهده تأثیر آن بر خروجی نهایی مدل، پارامترهای هر لایه را به‌طور مستقل استخراج می‌کند. این روش به‌گونه‌ای طراحی شده است که به عمق مدل بستگی ندارد و می‌تواند به‌صورت توزیع‌شده اجرا شود. آزمایش‌ها روی مدل **ResNet-18** نشان داده‌اند که الگوریتم SEEK می‌تواند با میانگین کمتر از **50 کوئری**، پارامترهای مدل را با خطای کمتر از **0.03٪** استخراج کند . این الگوریتم به دلیل کارآمدی بالا و توانایی شکستن پروتکل‌های امنیتی پیچیده، یک تهدید جدی برای سیستم‌های **پیش‌بینی به‌عنوان‌خدمت (Prediction as a Service - PaaS)** محسوب می‌شود و برای حسابرسی امنیت این سیستم‌ها بسیار مفید است.

#### 1.1.4. الگوریتم R.I.P. (Reusing of Incorrect Prediction)

الگوریتم **R.I.P. (Reusing of Incorrect Prediction)** یک حمله ساده و کارآمد علیه سیستم‌هایی است که در زمان آزمون (test-time) خود را با داده‌های بدون برچسب تطبیق می‌دهند (**Continual Test-time Adaptation - TTA**) . این حمله یک حمله جعبه سیاه (Black-Box) است، به این معنا که به اطلاعات داخلی مدل مانند پارامترها یا معماری آن نیاز ندارد. ایده اصلی این الگوریتم این است که نمونه‌هایی را که قبلاً توسط مدل به‌اشتباه طبقه‌بندی شده‌اند، دوباره برای تطبیق بعدی استفاده کند. این کار باعث می‌شود که مدل در اشتباهات خود تقویت شود و عملکرد آن به‌مرور زمان کاهش یابد. حمله R.I.P. با شناسایی نمونه‌هایی که به‌اشتباه در یک کلاس هدف قرار گرفته‌اند، آن‌ها را با نمونه‌های خوش‌نیت دیگر ترکیب کرده و به‌عنوان ورودی برای مرحله تطبیق بعدی استفاده می‌کند. این فرآیند تکراری باعث می‌شود که مدل به‌تدریج فروپاشی کند . این الگوریتم به دلیل سادگی و کاربرد عملی در سناریوهای واقعی، یک ابزار مهم برای حسابرسی امنیت و پایداری سیستم‌های TTA محسوب می‌شود.

### 1.2. الگوریتم‌های حمله عضویت (Membership Inference Attacks)

#### 1.2.1. الگوریتم LBRM (Loss-Based with Reference Model)

الگوریتم **LBRM (Loss-Based with Reference Model)** یک حمله عضویت (Membership Inference Attack - MIA) پیشرفته است که برای ارزیابی میزان حفظ حریم خصوصی در مدل‌های بازسازی سری زمانی (time series imputation models) طراحی شده است . این الگوریتم با استفاده از یک **مدل مرجع (reference model)** که روی یک مجموعه داده مشابه آموزش دیده است، دقت حمله را به‌طور قابل‌توجهی افزایش می‌دهد. ایده اصلی این است که تفاوت در رفتار مدل هدف و مدل مرجع روی یک نمونه داده، می‌تواند نشان‌دهنده عضویت یا عدم عضویت آن نمونه در مجموعه داده آموزش مدل هدف باشد. LBRM با تحلیل تغییرات در تابع زیان (loss function) و احتمالات خروجی مدل، تفاوت‌های ظریف بین داده‌های آموزشی و غیرآموزشی را شناسایی می‌کند. نتایج آزمایش‌ها نشان داده‌اند که این روش می‌تواند دقت حمله را به‌طور میانگین تا **40٪** بدون تنظیم دقیق (fine-tuning) و تا **60٪** با تنظیم دقیق افزایش دهد . این الگوریتم به دلیل کارآمدی بالا و توانایی شناسایی خطرات حریم خصوصی در مدل‌های پیچیده، یک ابزار مهم برای حسابرسی حریم خصوصی در کاربردهای حساس مانند پردازش داده‌های پزشکی و مالی است.

#### 1.2.2. الگوریتم SMIA (Semantic Membership Inference Attack)

الگوریتم **SMIA (Semantic Membership Inference Attack)** یک حمله عضویت نوین است که برای **مدل‌های زبان بزرگ (Large Language Models - LLMs)** طراحی شده است . برخلاف روش‌های سنتی که بیشتر بر حفظ حافظه کلمه‌به‌کلمه (verbatim memorization) تمرکز دارند، SMIA با استفاده از **محتوای معنایی ورودی و اختلالات (perturbations)** آن، عملکرد حمله را بهبود می‌بخشد. این الگوریتم با آموزش یک شبکه عصبی برای تحلیل رفتار مدل هدف روی ورودی‌های دست‌کاری‌شده، تفاوت‌های بین داده‌های عضو و غیرعضو را در توزیع احتمالات خروجی شناسایی می‌کند. SMIA به‌طور خاص برای مواردی طراحی شده است که LLMها داده‌های آموزشی خود را به‌صورت معنایی حفظ می‌کنند، نه لزوماً به‌صورت کلمه‌به‌کلمه. این روش با در نظر گرفتن پیچیدگی‌های زبان طبیعی، یک ارزیابی دقیق‌تر از خطرات حریم خصوصی در LLMها ارائه می‌دهد . این الگوریتم برای حسابرسی مدل‌های زبانی که در کاربردهای مختلفی مانند تولید متن، ترجمه و پاسخ‌گویی به سؤالات استفاده می‌شوند، بسیار مفید است.

#### 1.2.3. الگوریتم حمله عضویت مبتنی بر روند (Trend-based MIA)

الگوریتم حمله عضویت مبتنی بر روند (**Trend-based Membership Inference Attack**) یک روش برای استنباط عضویت است که از **تغییرات روند خروجی مدل در طول زمان** استفاده می‌کند . این روش بر این اساس استوار است که مدل‌ها معمولاً روی داده‌های آموزشی بیش از حد برازش (overfit) می‌کنند، که منجر به افزایش سریع‌تر نمره اطمینان (confidence score) یا کاهش سریع‌تر تابع زیان (loss) برای داده‌های آموزشی در مقایسه با داده‌های آزمون می‌شود. در این حمله، مهاجم یک دنباله از نسخه‌های مختلف مدل را جمع‌آوری می‌کند و روند تغییرات احتمال پیش‌بینی برچسب صحیح برای یک نمونه داده را تحلیل می‌کند. سپس یک مدل حمله (attack model) برای طبقه‌بندی این دنباله‌ها و تشخیص عضویت آموزش آموزش داده می‌شود . این روش به‌ویژه در **یادگیری فدرال (Federated Learning)** کاربرد دارد، جایی که مدل‌های محلی در طول زمان با سرور به‌روزرسانی می‌شوند. این الگوریتم یک ابزار مهم برای حسابرسی حریم خصوصی در سیستم‌های توزیع‌شده است.

### 1.3. الگوریتم‌های حسابرسی تقلبی و خرابکارانه (Adversarial & Evasion Auditing Algorithms)

#### 1.3.1. الگوریتم Two-Face

الگوریتم **Two-Face** یک روش حسابرسی خصمانه (Adversarial Audit) برای سیستم‌های تجاری تشخیص چهره است که در سال **2022** معرفی شد . هدف اصلی این الگوریتم، بررسی و افشای ناهنجاری‌ها و آسیب‌پذیری‌های موجود در APIهای سیستم‌های تشخیص چهره تجاری است. این روش با ارسال تصاویر یا درخواست‌های خاص به APIهای این سیستم‌ها، پاسخ‌های آن‌ها را تحلیل می‌کند تا تناقض‌ها یا رفتارهای غیرمنتظره را شناسایی کند. این نوع حسابرسی برای ارزیابی دقت، انصاف و امنیت مدل‌های تشخیص چهره که به صورت گسترده در کاربردهای مختلفی مانند احراز هویت، نظارت و بازاریابی استفاده می‌شوند، بسیار حیاتی است. Two-Face به عنوان یک ابزار حسابرسی مستقل عمل می‌کند که می‌تواند بدون نیاز به دسترسی داخلی به مدل یا داده‌های آموزشی آن، عملکرد سیستم را ارزیابی کند.

در یک مطالعه موردی، Two-Face برای حسابرسی چندین سیستم تشخیص چهره تجاری استفاده شد و نتایج نگران‌کننده‌ای را ارائه داد . این الگوریتم توانست تفاوت‌هایی در عملکرد سیستم‌ها نسبت به گروه‌های مختلف جمعیتی را شناسایی کند که می‌تواند نشان‌دهنده **تعصب (Bias)** در مدل باشد. همچنین، Two-Face قادر به کشف آسیب‌پذیری‌هایی بود که می‌توانستند منجر به فریب سیستم یا دسترسی غیرمجاز شوند. این یافته‌ها اهمیت استفاده از چنین ابزارهای حسابرسی را برای افزایش شفافیت و پاسخگویی در سیستم‌های هوش مصنوعی که بر زندگی روزمره افراد تأثیر می‌گذارند، برجسته می‌کند. با استفاده از روش‌های خصمانه، Two-Face می‌تواند نقاط ضعف مدل را که ممکن است در ارزیابی‌های استاندارد نادیده گرفته شوند، آشکار کند. این امر به توسعه‌دهندگان و سیاست‌گذاران کمک می‌کند تا درک بهتری از ریسک‌های مرتبط با استقرار این سیستم‌ها داشته باشند و اقدامات لازم برای بهبود امنیت و انصاف آن‌ها را انجام دهند.

#### 1.3.2. الگوریتم SCALE-UP (Black-box Input-level Backdoor Detection)

الگوریتم **SCALE-UP** یک روش کارآمد برای تشخیص **درب پشتی (Backdoor)** در سطح ورودی در مدل‌های یادگیری ماشین است که در یک محیط **جعبه سیاه (Black-box)** عمل می‌کند. این الگوریتم در مقاله‌ای با عنوان "SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency" در کنفرانس **ICLR 2022** معرفی شده است . این روش به‌طور خاص برای کاربردهای **خدمات یادگیری ماشین (MLaaS)** طراحی شده است، جایی که کاربران فقط به API مدل دسترسی دارند و نمی‌توانند به ساختار درونی آن نگاهی بیندازند. هدف اصلی SCALE-UP، شناسایی اینکه آیا یک مدل خاص دارای یک درب پشتی است یا نه، بدون نیاز به دسترسی به داده‌های آموزشی یا پارامترهای مدل. درب پشتی یک آسیب‌پذیری امنیتی است که در آن یک مهاجم می‌تواند با قرار دادن یک نشانگر خاص (trigger) در ورودی، رفتار مدل را به‌طور دلخواه تغییر دهد. این الگوریتم با تحلیل تغییرات در خروجی مدل در پاسخ به تغییرات مقیاس‌یافته در ورودی، این آسیب‌پذیری را شناسایی می‌کند.

روش کار SCALE-UP بر اساس این فرض است که مدل‌هایی که دارای درب پشتی هستند، نسبت به تغییرات مقیاس در ورودی‌های تمیز و آلوده، واکنش‌های متفاوتی نشان می‌دهند. به‌عبارت دیگر، اگر یک ورودی حاوی نشانگر درب پشتی باشد، تغییر مقیاس آن ممکن است تأثیر کمتری بر خروجی مدل داشته باشد یا حتی باعث شود که مدل به‌طور کامل رفتار خود را تغییر دهد. الگوریتم SCALE-UP با ایجاد مجموعه‌ای از ورودی‌های مشکوک و اعمال تغییرات مقیاس مختلف بر روی آن‌ها، میزان **سازگاری پیش‌بینی‌های مدل** را بررسی می‌کند. اگر سازگاری پیش‌بینی‌ها در برخی از ورودی‌ها به‌طور قابل‌توجهی کاهش یابد، این می‌تواند نشانه‌ای از وجود یک درب پشتی باشد. این روش به دلیل عدم نیاز به داده‌های آموزشی یا دسترسی به مدل داخلی، یک ابزار بسیار مفید برای حسابرسان و کاربران نهایی است که می‌خواهند امنیت مدل‌های MLaaS را ارزیابی کنند. پیاده‌سازی این الگوریتم با استفاده از ایجنتیک می‌تواند به‌طور خودکار و مداوم مدل‌های مختلف را مورد بررسی قرار دهد و گزارش‌هایی درباره هرگونه آسیب‌پذیری احتمالی ارائه دهد.

#### 1.3.3. الگوریتم GeoDA (Geometric Framework for Black-Box Adversarial Attacks)

الگوریتم **GeoDA (Geometric Framework for Black-Box Adversarial Attacks)** یکی از روش‌های پیشرفته برای حسابرسی مدل‌های یادگیری ماشین در برابر حملات تقلبی است، به‌ویژه در سناریوهای **جعبه سیاه** که ساختار داخلی مدل برای حسابرس ناشناخته است. این الگوریتم از یک چارچوب هندسی برای تولید نمونه‌های تقلبی استفاده می‌کند که قادر به فریب دادن مدل هدف هستند. برخلاف روش‌های سنتی که اغلب بر گرادیان‌های مدل تکیه دارند، GeoDA با استفاده از اصول هندسی، مسیرهای بهینه‌ای را برای تغییر ورودی‌ها پیدا می‌کند تا خروجی مدل را به اشتباه بیندازد. این رویکرد باعث می‌شود که GeoDA در مواردی که دسترسی به گرادیان‌ها ممکن نیست یا محدود است، بسیار مؤثر باشد. در یکی از منابع بررسی‌شده، به GeoDA به‌عنوان یکی از الگوریتم‌های کلیدی در حوزه حسابرسی الگوریتم‌ها اشاره شده است که در کنفرانس **CVPR** معرفی شده است . این الگوریتم با ارائه یک روش سیستماتیک برای یافتن نقاط ضعف مدل‌ها، به حسابرسان کمک می‌کند تا میزان مقاومت یک سیستم یادگیری ماشین را در برابر دستکاری‌های عمدی ارزیابی کنند. کاربرد اصلی آن در حسابرسی امنیت مدل‌های بینایی ماشین است، جایی که حتی تغییرات بسیار کوچک در تصاویر می‌تواند منجر به طبقه‌بندی نادرست شود. GeoDA با شناسایی این آسیب‌پذیری‌ها، به بهبود امنیت و قابلیت اطمینان سیستم‌های هوش مصنوعی کمک می‌کند.

## 2. الگوریتم‌های حسابرسی عادلانه و حریم خصوصی (Fairness & Privacy Auditing Algorithms)

### 2.1. الگوریتم‌های حسابرسی عادلانه (Fairness Auditing Algorithms)

#### 2.1.1. الگوریتم FairProof

الگوریتم **FairProof** یک سیستم نوآورانه برای ارائه **گواهی انصاف (Fairness Certificate)** برای شبکه‌های عصبی است که از **اثبات دانش صفر (Zero-Knowledge Proofs - ZKPs)** برای حفظ محرمانگی مدل استفاده می‌کند . این الگوریتم به طور خاص برای مواردی طراحی شده است که مدل‌های یادگیری ماشین به دلایل قانونی یا مالکیت معنوی باید محرمانه باقی بمانند، اما در عین حال نیاز به تأیید عمومی در مورد خواص مطلوب مانند انصاف دارند . FairProof از یک معیار انصاف محلی به نام **"انصاف فردی" (Individual Fairness - IF)** استفاده می‌کند که انصاف مدل را در یک نقطه داده خاص ارزیابی می‌کند. این ویژگی امکان ارائه گواهی شخصی‌سازی شده به هر مشتری را فراهم می‌کند که برای سازمان‌های مشتری‌محور بسیار مفید است . الگوریتم FairProof از دو بخش اصلی تشکیل شده است: یک الگوریتم گواهی‌سازی انصاف که گواهی‌نامه را تولید می‌کند و یک پروتکل رمزنگاری که با استفاده از تعهدات (Commitments) و ZKPs، یکنواختی مدل را تضمین کرده و اثبات صحت گواهی‌نامه را ارائه می‌دهد .

فرآیند کار FairProof به این صورت است که یک "اثبات‌کننده" (Prover)، مانند یک بانک، گواهی‌نامه انصاف را برای یک مدل خاص محاسبه می‌کند. سپس، با استفاده از ZKPs، یک "بازرس" (Verifier)، مانند یک مشتری، می‌تواند صحت این گواهی‌نامه را بدون دسترسی به وزن‌های واقعی مدل تأیید کند . این کار از طریق کاهش مسئله گواهی‌سازی انصاف به یک مسئله اثبات پایداری (Certifiable Robustness) انجام می‌شود. الگوریتم به گونه‌ای طراحی شده است که با ZKPs سازگار باشد تا هزینه محاسباتی را کاهش دهد. برای مثال، در یک آزمایش با داده‌های **German Credit**، FairProof به طور میانگین حدود **1.17 دقیقه** برای تولید یک گواهی انصاف قابل تأیید برای هر نقطه داده زمان می‌برد، در حالی که اندازه گواهی‌نامه تنها **43.5 کیلوبایت** است . این ویژگی‌ها FairProof را به یک راه‌حل عملی و کارآمد برای حسابرسی انصاف مدل‌های محرمانه تبدیل می‌کنند.

#### 2.1.2. الگوریتم AVOIR (Online Fairness Auditing through Iterative Refinement)

الگوریتم **AVOIR (Online Fairness Auditing through Iterative Refinement)** یک سیستم پیشرفته برای حسابرسی آنلاین انصاف در مدل‌های یادگیری ماشین است که به صورت جعبه سیاه عمل می‌کنند . این الگوریتم که در کنفرانس **KDD 2023** معرفی شد، با استفاده از یک فرآیند تطبیقی (Adaptive Process) و استدلال احتمالاتی (Probabilistic Inference)، نظارت بر خواص انصاف مدل در زمان اجرا (Runtime) را ممکن می‌سازد . هدف اصلی AVOIR کاهش تعداد مشاهدات مورد نیاز برای نظارت بر اظهارات احتمالاتی مربوط به معیارهای انصاف است. این الگوریتم قادر است تضمین‌های احتمالاتی مرتبط با تخمین طیف گسترده‌ای از معیارهای انصاف را به صورت خودکار استنباط کند. علاوه بر این، AVOIR امکان بررسی نقض‌های انصاف را فراهم می‌کند که با الزامات حکومتی و نظارتی هماهنگ هستند . این ویژگی‌ها AVOIR را به یک ابزار قدرتمند برای حسابرسی مدل‌هایی که در حال حاضر در حال استفاده هستند و نیاز به نظارت مستمر دارند، تبدیل می‌کند.

AVOIR از طریق یک کتابخانه Python پیاده‌سازی شده است که از روش‌هایی مانند **نمونه‌گیری رد (Rejection Sampling)** برای تخمین احتمالات شرطی و ابزارهای بهینه‌سازی مانند **IPOPT** از طریق رابط Pyomo استفاده می‌کند . در مطالعات موردی که با سه مجموعه داده واقعی انجام شد، AVOIR توانست نقض‌های انصاف را شناسایی و محلی‌سازی کند و همچنین مشکلات مربوط به طراحی نادرست معیارهای انصاف را برطرف کند. برای مثال، در یک مطالعه موردی با داده‌های **Rate My Professors**، AVOIR یک مدل مبتنی بر BERT را از نظر انصاف جنسیتی حسابرسی کرد. نتایج نشان داد که نسخه بهینه‌شده AVOIR (AVOIR-OB) می‌تواند تضمین‌های لازم را با **2.5٪ تکرار کمتر** نسبت به نسخه پایه (AVOIR-VF) ارائه دهد . این کارایی بالا AVOIR را به یک راه‌حل مناسب برای حسابرسی مقیاس‌پذیر و کارآمد مدل‌های پیچیده در دنیای واقعی تبدیل می‌کند.

#### 2.1.3. الگوریتم Auditing Fairness by Betting

الگوریتم **"Auditing Fairness by Betting"** یک روش نوآورانه برای حسابرسی عادلانه است که در مقاله‌ای با همین عنوان در کنفرانس **NeurIPS** معرفی شده است . این الگوریتم از یک چارچوب نظری به نام **"شرط‌بندی سریالی" (sequential betting)** برای ارزیابی عادلانه بودن یک مدل یادگیری ماشین استفاده می‌کند. این روش به‌جای استفاده از روش‌های سنتی آمار کلاسیک که معمولاً بر اساس فرضیات خاصی درباره توزیع داده‌ها هستند، یک رویکرد غیرپارامتریک و انطباق‌پذیر ارائه می‌دهد. ایده اصلی این الگوریتم این است که یک "بازیکن" (player) در یک بازی شرط‌بندی شرکت می‌کند و در هر مرحله، بر اساس مشاهدات قبلی، شرطی بر روی اینکه آیا مدل در مورد یک نمونه جدید عادلانه عمل خواهد کرد یا نه، می‌بندد. اگر مدل در طول زمان به‌طور مداوم عادلانه عمل کند، ثروت بازیکن به‌طور تصادفی نوسان خواهد کرد. اما اگر مدل ناعادلانه باشد، بازیکن می‌تواند با استفاده از یک استراتژی شرط‌بندی مناسب، ثروت خود را به‌طور سیستماتیک افزایش دهد.

این الگوریتم به‌ویژه برای **حسابرسی در زمان واقعی (online auditing)** مناسب است، زیرا می‌تواند به‌صورت مداوم داده‌های جدید را پردازش کند و در صورت مشاهده هرگونه شواهد قوی از ناعادلانه بودن، هشدار دهد. یکی از مزایای اصلی این روش، توانایی آن در ارائه **تضمین‌های آماری قوی** بدون نیاز به دانستن اندازه کل جمعیت یا توزیع داده‌ها است. همچنین، این الگوریتم می‌تواند با استفاده از روش‌های "anytime valid"، در هر نقطه از فرآیند حسابرسی، یک تخمین معتبر از میزان عادلانه بودن ارائه دهد. این ویژگی‌ها آن را به یک ابزار بسیار قدرتمند برای حسابرسان نظارتی تبدیل می‌کند که نیاز به تصمیم‌گیری سریع و مبتنی بر شواهد دارند. پیاده‌سازی این الگوریتم با استفاده از ایجنتیک می‌تواند به‌طور خودکار این فرآیند شرط‌بندی سریالی را انجام دهد و گزارش‌هایی درباره وضعیت عادلانه بودن مدل در زمان واقعی ارائه دهد.

#### 2.1.4. الگوریتم Active Fairness Auditing

الگوریتم **Active Fairness Auditing** یک روش مبتنی بر **پرس‌وجو (Query-Based)** برای تخمین کارآمد معیارهای انصاف در مدل‌های یادگیری ماشین است . این الگوریتم که در سال **2022** معرفی شد، برای مواردی طراحی شده است که یک ناظر یا رگولاتور می‌خواهد انصاف یک مدل را حسابرسی کند، اما منابع محدودی برای پرس‌وجو از مدل دارد. Active Fairness Auditing با استفاده از استراتژی‌های **یادگیری فعال (Active Learning)** ، تلاش می‌کند تا با حداقل تعداد پرس‌وجو، برآوردی دقیق از معیار انصاف مانند **"برابری جمعیتی" (Demographic Parity)** ارائه دهد. این الگوریتم دو نوع تضمین را ارائه می‌دهد: دقت برآورد مستقیم و **"اثبات‌پذیری در برابر دستکاری" (Manipulation-Proofness)** . این ویژگی دوم به این معناست که حتی اگر شرکت پس از اتمام حسابرسی، مدل خود را تغییر دهد، برآورد انجام شده همچنان معتبر خواهد بود.

الگوریتم Active Fairness Auditing از یک مدل یادگیری فعال با بودجه برچسب (Label Budget) استفاده می‌کند. در این مدل، الگوریتم در هر مرحله یک نمونه بدون برچسب را انتخاب می‌کند و برچسب آن را از مدل هدف دریافت می‌کند. این فرآیند تا زمانی که بودجه پرس‌وجو تمام شود ادامه می‌یابد. سپس، بر اساس مجموعه‌ای از نمونه‌های برچسب‌خورده، معیار انصاف تخمین زده می‌شود. برای دستیابی به اثبات‌پذیری در برابر دستکاری، الگوریتم باید مجموعه‌ای از پرس‌وجوها را انتخاب کند که به عنوان یک **"گواهی" (Certificate)** عمل کنند. این گواهی تضمین می‌کند که برآورد انصاف برای هر مدلی که با نتایج پرس‌وجوها سازگار است، در یک حاشیه خطای مشخص باقی خواهد ماند . این الگوریتم همچنین به بررسی پیچیدگی پرس‌وجوی بهینه برای الگوریتم‌های تصادفی می‌پردازد و تلاش می‌کند تا پایه‌ای نظری برای حکمرانی هوش مصنوعی فراهم کند .

#### 2.1.5. الگوریتم The Fair Game

الگوریتم **The Fair Game** یک چارچوب نوآورانه برای حسابرسی و کاهش تعصب (Debiasing) الگوریتم‌های هوش مصنوعی در طول زمان است. این الگوریتم که در قالب یک ارائه در **Cambridge Forum on AI: Law and Governance** معرفی شد، هدف آن شبیه‌سازی تکامل چارچوب‌های اخلاقی و قانونی در جامعه از طریق ایجاد یک حسابرس است که بازخورد را به یک الگوریتم حذف تعصب که در اطراف یک سیستم یادگیری ماشین مستقر شده است، ارسال می‌کند . این الگوریتم با الهام از مفاهیم **یادگیری تقویتی (Reinforcement Learning)** طراحی شده است تا بتواند در یک محیط پویا و در حال تغییر، عادلانه بودن را حفظ کند. برخلاف روش‌های سنتی که معمولاً برای محیط‌های ایستاتیک طراحی شده‌اند، "The Fair Game" یک حلقه بازخورد بین یک **حسابرس (Auditor)** و یک **الگوریتم رفع سوگیری (Debiasing Algorithm)** ایجاد می‌کند. حسابرس به‌طور مداوم سیستم را ارزیابی می‌کند و بازخورد خود را درباره هرگونه سوگیری شناسایی‌شده به الگوریتم رفع سوگیری ارائه می‌دهد. این الگوریتم نیز با استفاده از این بازخورد، مدل را به‌صورت پویا تنظیم می‌کند تا سوگیری را کاهش دهد. این فرآیند تعاملی و تکراری باعث می‌شود که سیستم بتواند خود را با تغییرات در داده‌ها یا الزامات اجتماعی تطبیق دهد. الگوریتم "The Fair Game" بر ویژگی‌های مطلوبی مانند دقت، صرفه‌جویی در داده و مقاومت در برابر دستکاری تأکید دارد. پیاده‌سازی این الگوریتم به‌صورت ایجنتیک می‌تواند منجر به ایجاد یک سیستم خودکار و هوشمند شود که نه‌تنها سوگیری‌ها را شناسایی می‌کند، بلکه به‌طور فعال برای رفع آن‌ها اقدام می‌کند، که این امر باعث افزایش اعتماد و پاسخ‌گویی در سیستم‌های هوش مصنوعی می‌شود .

### 2.2. الگوریتم‌های حسابرسی حریم خصوصی (Privacy Auditing Algorithms)

#### 2.2.1. الگوریتم P2NIA (Privacy-Preserving Non-Iterative Auditing)

الگوریتم **P2NIA (Privacy-Preserving Non-Iterative Auditing)** یک طرح حسابرسی نوآورانه است که برای ارزیابی انطباق اخلاقی سیستم‌های هوش مصنوعی پرخطر طراحی شده است . این الگوریتم که در سال **2025** معرفی شد، به چالش‌های موجود در روش‌های حسابرسی سنتی که بر APIهای پلتفرم‌ها تکیه دارند، پاسخ می‌دهد. این روش‌های سنتی باعث ایجاد بار سنگینی بر پلتفرم‌ها می‌شوند، زیرا آن‌ها مجبور به نگهداری APIها هستند و در عین حال باید حریم خصوصی را حفظ کرده و از نشت داده‌ها جلوگیری کنند. علاوه بر این، عدم همکاری مناسب بین پلتفرم و حسابرس می‌تواند منجر به سوگیری در برآورد حسابرس شود، زیرا حسابرس از توزیع داده‌های پلتفرم اطلاعی ندارد. P2NIA با ارائه یک طرح همکاری متقابل که برای هر دو طرف سودمند است، این مشکلات را برطرف می‌کند .

در طرح P2NIA، فرآیند حسابرسی در شش مرحله انجام می‌شود. ابتدا، حسابرس تعداد پرس‌وجوهای مورد نظر و گروه محافظت‌شده را مشخص می‌کند. سپس، پلتفرم با استفاده از مدل اختصاصی خود، بخشی از داده‌های داخلی خود را برچسب‌گذاری کرده و یک مجموعه داده حسابرسی ایجاد می‌کند. برای حفاظت از اطلاعات حساس، پلتفرم دو گزینه دارد: یا از یک مکانیزم حفظ حریم خصوصی مبتنی بر **اختلاف محلی (Local Differential Privacy)** برای ناشناس‌سازی داده‌ها استفاده می‌کند یا یک مدل تولیدی آموزش می‌بیند تا داده‌های مصنوعی تولید کند که رفتار مدل را تقلید کند. در نهایت، این مجموعه داده مصنوعی یا ناشناس‌سازی شده به حسابرس منتقل می‌شود تا ارزیابی انصاف بر روی آن انجام شود . آزمایشات گسترده نشان داده‌اند که P2NIA حتی با اندازه نمونه متوسط نیز دقت بالایی را ارائه می‌دهد و برتری خود را نسبت به روش‌های جعبه سیاه سنتی که به داده‌های بزرگ و مستعد سوگیری نیاز دارند، ثابت می‌کند .

#### 2.2.2. الگوریتم Privacy Oracle

الگوریتم **Privacy Oracle** یک روش برای حسابرسی حریم خصوصی در سیستم‌های یادگیری ماشین است که به‌ویژه برای ارزیابی **حریم خصوصی تفاضلی (Differential Privacy)** طراحی شده است. این الگوریتم به عنوان یک "اوراکل" عمل می‌کند که می‌تواند به سؤالات حسابرس درباره سطح حفظ حریم خصوصی یک سیستم پاسخ دهد. در یکی از پژوهش‌های اخیر، یک چارچوب مفهومی جدید برای حسابرسی حریم خصوصی تفاضلی در سناریوهای جعبه سیاه معرفی شده است که ماهیت خصمانه ذاتی در حسابرسی حریم خصوصی را به‌طور صریح در نظر می‌گیرد . این چارچوب شامل یک الگوریتم عملی و کارآمد از نظر محاسباتی است که از اصول **استنباج همگرا (conformal inference)** بهره می‌برد و قادر است به‌طور قوی خطای نوع I را در حالت نمونه‌های محدود کنترل کند، بدون اینکه به فرض‌های محدودکننده نیاز داشته باشد. این الگوریتم همچنین قادر به ساخت یک **باند اطمینان معتبر** برای تابع trade-off در حالت جعبه سیاه است که دارای اعتبار نمونه محدود است . این روش‌ها با ارائه ضمانت‌های عملکرد قوی از نظر ریاضی، به حسابرسان اجازه می‌دهند تا ادعاهای مربوط به حفظ حریم خصوصی یک سیستم را به‌طور مستقل و معتبر ارزیابی کنند.

#### 2.2.3. الگوریتم Auditing with One (1) Training Run

الگوریتم **"Auditing with One (1) Training Run"** یک طرح برای حسابرسی سیستم‌های یادگیری ماشین با حفظ حریم خصوصی دیفرانسیلی (Differentially Private) است که در سال **2023** در کنفرانس **NeurIPS** معرفی شد. این الگوریتم برنده جایزه بهترین مقاله در این کنفرانس شد. همانطور که از نامش پیداست، این روش تنها با **یک بار اجرای آموزش (Training Run)** امکان حسابرسی را فراهم می‌کند. این ویژگی آن را بسیار کارآمد و مقرون به صرفه می‌سازد. الگوریتم Auditing with One (1) Training Run در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی حریم خصوصی ذکر شده است . این روش برای حسابرسی مدل‌هایی که روی داده‌های حساس آموزش دیده‌اند، مانند داده‌های پزشکی یا مالی، بسیار مفید است.

## 3. الگوریتم‌های حسابرسی شفافیت و تفسیرپذیری (Transparency & Interpretability Auditing Algorithms)

### 3.1. الگوریتم‌های حسابرسی جعبه سیاه (Black-Box Auditing Algorithms)

#### 3.1.1. الگوریتم CALM (Curiosity-Driven Auditing for Large Language Models)

الگوریتم **CALM (Curiosity-Driven Auditing for Large Language Models)** یک روش حسابرسی برای مدل‌های زبانی بزرگ (LLMs) است که در سال **2025** در کنفرانس **AAAI** معرفی شد. این الگوریتم حسابرسی را به عنوان یک مسئله بهینه‌سازی جعبه سیاه در نظر می‌گیرد که هدف آن به طور خودکار کشف جفت‌های ورودی-خروجی از مدل هدف است که رفتارهای غیرقانونی، غیراخلاقی یا ناامن را نشان می‌دهند. CALM با استفاده از یک **مکانیزم کنجکاوی‌محور (curiosity-driven mechanism)** ، به طور فعال به دنبال نقاط ضعف و رفتارهای نامطلوب در مدل‌های LLM می‌گردد. این الگوریتم در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی مدل‌های LLM ذکر شده است . این روش به حسابرسان کمک می‌کند تا بدون نیاز به دسترسی داخلی به مدل، رفتارهای خطرناک یا نادرست آن را شناسایی کنند.

#### 3.1.2. الگوریتم Auditing Pay-Per-Token in Large Language Models

الگوریتم **Auditing Pay-Per-Token in Large Language Models** یک چارچوب حسابرسی است که در سال **2025** در arXiv معرفی شد. این الگوریتم بر اساس **نظریه مارتینگل (Martingale Theory)** توسعه یافته است و به یک حسابرس شخص ثالث قابل اعتماد امکان می‌دهد تا با ارسال کوئری‌های متوالی به یک ارائه‌دهنده، گزارش‌دهی نادرست توکن‌ها را تشخیص دهد. این روش برای حسابرسی مدل‌های زبانی بزرگی که بر اساس پرداخت به ازای هر توکن (Pay-Per-Token) کار می‌کنند، بسیار مفید است. الگوریتم Auditing Pay-Per-Token in Large Language Models در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی مدل‌های LLM ذکر شده است . این الگوریتم به حسابرسان کمک می‌کند تا از صحت صورتحساب‌های ارائه‌شده توسط ارائه‌دهندگان خدمات LLM اطمینان حاصل کنند.

#### 3.1.3. الگوریتم Robust ML Auditing Using Prior Knowledge

الگوریتم **Robust ML Auditing Using Prior Knowledge** یک روش برای حسابرسی قوی مدل‌های یادگیری ماشین است که در سال **2025** در کنفرانس **ICML** معرفی شد. این الگوریتم به طور رسمی شرایطی را تعیین می‌کند که در آن یک حسابرس می‌تواند با استفاده از دانش قبلی در مورد حقیقت زمینه‌ای (Ground Truth)، دستکاری در حسابرسی را جلوگیری کند. این روش با ترکیب دانش قبلی، حسابرسی را قوی‌تر و قابل اعتمادتر می‌سازد. الگوریتم Robust ML Auditing Using Prior Knowledge در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی قوی ذکر شده است . این الگوریتم برای حسابرسی مدل‌هایی که در معرض دستکاری یا حملات خصمانه هستند، بسیار مفید است.

#### 3.1.4. الگوریتم XAudit (Auditing with Explanations)

الگوریتم **XAudit** یک نگاه نظری به حسابرسی با استفاده از **توضیحات (Explanations)** است که در سال **2023** در arXiv معرفی شد. این الگوریتم نقش توضیحات در حسابرسی را به صورت رسمی تعریف می‌کند و بررسی می‌کند که آیا و چگونه توضیحات مدل می‌توانند به حسابرسی کمک کنند. XAudit با تحلیل توانایی توضیحات در کشف خطا و تعصب، بینش‌های ارزشمندی در مورد ارتباط بین تفسیرپذیری و حسابرسی ارائه می‌دهد. این الگوریتم در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی با استفاده از توضیحات ذکر شده است . این روش به حسابرسان کمک می‌کند تا با استفاده از توضیحات مدل، دلایل تصمیم‌گیری آن را درک کرده و خطاها یا تعصب‌های احتمالی را شناسایی کنند.

### 3.2. الگوریتم‌های حسابرسی توصیه‌گرها و سیستم‌های جستجو (Recommender & Search System Auditing)

#### 3.2.1. الگوریتم Auditing YouTube’s Recommendation Algorithm

الگوریتم **Auditing YouTube’s Recommendation Algorithm** یک روش برای حسابرسی الگوریتم توصیه‌گر یوتیوب است که در سال **2023** در ژورنال **Transactions on Recommender Systems** معرفی شد. این الگوریتم بر روی تشخیص **حباب‌های اطلاعات نادرست (Misinformation Filter Bubbles)** در توصیه‌های یوتیوب تمرکز دارد. هدف این الگوریتم بررسی این است که چه عواملی می‌توانند "حباب" را بترکانند، یعنی محصور شدن در توصیه‌ها را معکوس کنند. این الگوریتم در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی سیستم‌های توصیه‌گر ذکر شده است . این روش به حسابرسان کمک می‌کند تا درک کنند که الگوریتم توصیه‌گر یوتیوب چگونه می‌تواند کاربران را در یک حلقه بازخورد از محتوای مشابه محصور کند.

#### 3.2.2. الگوریتم Auditing Yelp’s Business Ranking and Review Recommender Systems

الگوریتم **Auditing Yelp’s Business Ranking and Review Recommender Systems** یک روش برای حسابرسی سیستم‌های رتبه‌بندی کسب‌وکار و توصیه‌گر نظرات در Yelp است که در سال **2023** در arXiv معرفی شد. این الگوریتم عادلانه بودن این سیستم‌ها را از دیدگاه **برابری جمعیتی (Demographic Parity)** ، **نمایش (Exposure)** و آزمون‌های آماری مانند رگرسیون خطی و لجستیک کمیته بررسی می‌کند. این الگوریتم در مخزن "Awesome Audit Algorithms" در گیت‌هاب به عنوان یکی از روش‌های مهم در حوزه حسابرسی سیستم‌های توصیه‌گر ذکر شده است . این روش به حسابرسان کمک می‌کند تا بررسی کنند که آیا سیستم‌های توصیه‌گر Yelp به‌طور عادلانه به همه کسب‌وکارها و نظرات کاربران رسیدگی می‌کنند یا خیر.

#### 3.2.3. الگوریتم Measuring Personalization of Web Search

الگوریتم **Measuring Personalization of Web Search** یک روش برای اندازه‌گیری میزان شخصی‌سازی در نتایج جستجوی وب است. این الگوریتم با استفاده از حساب‌های کاربری جعلی (sock puppets) یا کاربران واقعی، نتایج جستجوی یکسان را برای پروفایل‌های مختلف مقایسه می‌کند. تفاوت در نتایج جستجو می‌تواند نشان‌دهنده میزان شخصی‌سازی الگوریتم جستجو باشد. این روش به حسابرسان کمک می‌کند تا درک کنند که موتورهای جستجو چگونه بر اساس تاریخچه جستجو، مکان جغرافیایی، و سایر ویژگی‌های کاربر، نتایج را تغییر می‌دهند. این اطلاعات برای ارزیابی شفافیت و عادلانه بودن الگوریتم‌های جستجو بسیار مهم است.

## 4. الگوریتم‌های حسابرسی مبتنی بر عامل (Agent-Based Auditing Algorithms)

### 4.1. الگوریتم‌های حسابرسی چندعاملی (Multi-Agent Auditing)

#### 4.1.1. الگوریتم Fairness Auditing with Multi-Agent Collaboration

الگوریتم **Fairness Auditing with Multi-Agent Collaboration** یک چارچوب نوین برای حسابرسی عادلانه ارائه می‌دهد که در آن چندین عامل هوش مصنوعی با همکاری یکدیگر، یک پلتفرم مشترک را برای وظایف مختلف حسابرسی می‌کنند. در این مدل، هر عامل ممکن است مسئولیت بررسی یک جنبه خاص از عادلانه بودن را داشته باشد، مانند بررسی سوگیری نژادی، جنسیتی یا اقتصادی. با همکاری و تبادل اطلاعات بین این عوامل، یک تصویر جامع‌تر و دقیق‌تر از وضعیت عادلانه بودن سیستم به‌دست می‌آید. این رویکرد چندعاملی مزایای قابل‌توجهی دارد. اولاً، با تقسیم کار بین چندین عامل، می‌توان فرآیند حسابرسی را تسریع کرد و آن را مقیاس‌پذیرتر کرد. ثانیاً، استفاده از چندین عامل با دیدگاه‌ها و تخصص‌های مختلف می‌تواند منجر به شناسایی دقیق‌تر و جامع‌تر سوگیری‌ها شود. برای مثال، یک عامل ممکن است در شناسایی الگوهای پیچیده در داده‌ها تخصص داشته باشد، در حالی که عامل دیگر ممکن است در تفسیر قوانین و مقررات مربوط به عادلانه بودن مهارت داشته باشد. این الگوریتم به‌ویژه برای حسابرسی سیستم‌های بزرگ و پیچیده که دارای ابعاد مختلفی از عادلانه بودن هستند، مناسب است. پیاده‌سازی این الگوریتم به‌صورت ایجنتیک می‌تواند منجر به ایجاد یک تیم خودکار از حسابرسان هوش مصنوعی شود که به‌طور هماهنگ و کارآمد، عادلانه بودن سیستم‌های مختلف را ارزیابی می‌کنند .

#### 4.1.2. الگوریتم FACT-AUDIT (Multi-Agent Framework for Dynamic Fact-Checking)

الگوریتم **FACT-AUDIT** یک چارچوب چندعاملی تطبیقی برای ارزیابی پویای بررسی واقعیت (fact-checking) در مدل‌های زبان بزرگ (LLM) ارائه می‌دهد. این الگوریتم با استفاده از چندین عامل با نقش‌های مختلف، به‌طور سیستماتیک توانایی‌های بررسی واقعیت یک مدل LLM را آزمایش می‌کند و محدودیت‌های آن را شناسایی می‌کند. چارچوب FACT-AUDIT شامل سه مرحله اصلی است: **شبیه‌سازی نمونه اولیه (Prototype Emulation)** ، **تأیید واقعیت با توجیه (Fact Verification with Justification)** و **به‌روزرسانی تطبیقی (Adaptive Updating)** . در مرحله اول، عامل‌های "Appraiser" و "Inquirer" با همکاری یکدیگر، سناریوهای آزمایشی را ایجاد می‌کنند. در مرحله دوم، عامل "Prober" این سناریوها را به مدل LLM ارائه می‌دهد و پاسخ‌های آن را بررسی می‌کند. در نهایت، در مرحله سوم، اطلاعات به‌دست‌آمده از این فرآیند برای به‌روزرسانی و بهبود سناریوهای آزمایشی در دوره‌های بعدی استفاده می‌شود. این رویکرد تطبیقی باعث می‌شود که الگوریتم بتواند به‌طور مداوم توانایی‌های مدل LLM را در بررسی واقعیت ارزیابی کرده و نقاط ضعف آن را شناسایی کند. پیاده‌سازی این الگوریتم به‌صورت ایجنتیک می‌تواند منجر به ایجاد یک سیستم خودکار شود که به‌طور مستمر دقت و اعتبار اطلاعات ارائه‌شده توسط مدل‌های زبان بزرگ را نظارت می‌کند .

### 4.2. الگوریتم‌های حسابرسی مستقل و خودکار (Autonomous & Automated Auditing)

#### 4.2.1. الگوریتم RepoAudit (Autonomous LLM-Driven Agent)

الگوریتم **RepoAudit** یک عامل خودکار و مبتنی بر **مدل زبان بزرگ (LLM)** است که برای حسابرسی خودکار مخازن کد طراحی شده است. این عامل می‌تواند به‌طور مستقل یک مخزن کد را بررسی کرده و مشکلات امنیتی، کیفیت کد و رعایت استانداردها را شناسایی کند. RepoAudit با استفاده از قابلیت‌های درک زبان طبیعی و استدلال مدل‌های LLM، می‌تواند کد را تجزیه و تحلیل کرده و آسیب‌پذیری‌های بالقوه را شناسایی کند. این عامل می‌تواند به‌صورت خودکار گزارش‌هایی را تهیه کند که شامل جزئیات مشکلات شناسایی‌شده، توصیه‌هایی برای رفع آن‌ها و اطلاعات مربوط به ریسک‌های امنیتی باشد. الگوریتم RepoAudit می‌تواند در چرخه توسعه نرم‌افزار (CI/CD) یکپارچه شود تا به‌صورت مداوم کیفیت و امنیت کد را تضمین کند. این رویکرد خودکار باعث کاهش هزینه‌ها و زمان مورد نیاز برای حسابرسی دستی می‌شود و همچنین دقت و پوشش حسابرسی را افزایش می‌دهد. پیاده‌سازی این الگوریتم به‌صورت ایجنتیک می‌تواند منجر به ایجاد یک ابزار قدرتمند شود که به تیم‌های توسعه نرم‌افزار کمک می‌کند تا نرم‌افزارهای ایمن‌تر و با کیفیت‌تری را تولید کنند.

#### 4.2.2. الگوریتم Sock-puppet audit

الگوریتم **Sock-puppet audit** یک روش خودکار برای حسابرسی سیستم‌های الگوریتمی است که در آن از برنامه‌های کامپیوتری برای تقلید از کاربران در یک پلتفرم استفاده می‌شود. این برنامه‌های کامپیوتری که **"sock puppets"** نامیده می‌شوند، با پلتفرم تعامل می‌کنند و داده‌های تولیدشده توسط پلتفرم در پاسخ به این کاربران تقلیدی ثبت و تحلیل می‌شوند . این روش به حسابرسان کمک می‌کند تا درک کنند که یک پروفایل خاص یا مجموعه‌ای از پروفایل‌های کاربری ممکن است چه تجربه‌ای را در یک پلتفرم داشته باشند. برای مثال، می‌توان از sock puppets برای بررسی این موضوع استفاده کرد که آیا الگوریتم توصیه‌گر یک وب‌سایت خبری محتوای خاصی را به کاربران با ویژگی‌های جمعیتی خاص نشان می‌دهد یا خیر. این روش به‌ویژه برای حسابرسی سیستم‌هایی که دارای شخصی‌سازی بالایی هستند، مفید است، زیرا امکان مقایسه تجربیات مختلف کاربران را فراهم می‌کند. Sock-puppet audit یکی از شش روش شناسایی‌شده در یک گزارش از موسسه **Ada Lovelace Institute** برای بازرسی نظارتی از سیستم‌های الگوریتمی است . این روش با ارائه یک روش سیستماتیک برای جمع‌آوری داده‌ها از یک پلتفرم، به حسابرسان اجازه می‌دهد تا به‌طور عینی رفتار الگوریتم‌ها را ارزیابی کنند.

#### 4.2.3. الگوریتم Crowd-sourced audit

الگوریتم **Crowd-sourced audit**، که گاهی به آن "mystery-shopper" audit نیز گفته می‌شود، یک روش برای حسابرسی سیستم‌های الگوریتمی است که در آن از کاربران واقعی برای جمع‌آوری اطلاعات از یک پلتفرم استفاده می‌شود. این کاربران ممکن است تجربه خود را به‌صورت دستی گزارش دهند یا از ابزارهای خودکار مانند افزونه‌های مرورگر برای ثبت داده‌ها استفاده کنند . این روش به حسابرسان اجازه می‌دهد تا مشاهده کنند که کاربران واقعی چه محتوایی را در یک پلتفرم تجربه می‌کنند و آیا پروفایل‌های مختلف کاربری محتوای متفاوتی را دریافت می‌کنند یا خیر. Crowd-sourced audit یکی از شش روش شناسایی‌شده در یک گزارش از موسسه **Ada Lovelace Institute** برای بازرسی نظارتی از سیستم‌های الگوریتمی است . این روش با بهره‌گیری از توان جمعی کاربران، می‌تواند داده‌هایی را فراهم کند که با روش‌های خودکار قابل دستیابی نیستند. برای مثال، می‌توان از این روش برای بررسی این موضوع استفاده کرد که آیا یک پلتفرم تجارت الکترونیک قیمت‌های مختلفی را به کاربران در مکان‌های جغرافیایی مختلف نشان می‌دهد یا خیر. این روش با ارائه یک روش مشارکتی برای حسابرسی، به حسابرسان کمک می‌کند تا دیدگاه‌های واقعی کاربران را در فرآیند ارزیابی لحاظ کنند.

## 5. الگوریتم‌های کلاسیک و کاربردی در حسابرسی (Classic & Applied Algorithms in Auditing)

### 5.1. الگوریتم‌های داده‌کاوی در حسابرسی (Data Mining Algorithms in Auditing)

#### 5.1.1. الگوریتم درخت تصمیم (Decision Trees)

الگوریتم **درخت تصمیم** یکی از الگوریتم‌های کلاسیک یادگیری ماشین است که در حسابرسی برای طبقه‌بندی و پیش‌بینی استفاده می‌شود. این الگوریتم با ایجاد یک مدل به شکل درخت، تصمیم‌گیری‌ها را به‌صورت مجموعه‌ای از قوانین if-then نمایش می‌دهد. در یک گزارش حسابرسی از دولت هلند، درخت تصمیم به‌عنوان یکی از الگوریتم‌های "فنی ساده" شناخته شده است که در سیستم‌های دولتی برای ارزیابی درخواست‌ها و طبقه‌بندی ریسک استفاده می‌شود . برای مثال، در یکی از موارد بررسی‌شده، یک درخت تصمیم برای کمک به کارکنان در ارزیابی درخواست‌های کمک هزینه مسکن استفاده شده است. **سادگی تفسیر** این الگوریتم یکی از مزایای اصلی آن در حسابرسی است، زیرا به حسابرسان اجازه می‌دهد تا به‌راحتی منطق تصمیم‌گیری الگوریتم را درک و ارزیابی کنند. با این حال، این الگوریتم ممکن است در برابر داده‌های پیچیده و دارای نویز حساس باشد و ممکن است به‌راحتی دچار overfitting شود. در حسابرسی، این الگوریتم می‌تواند برای شناسایی الگوهای مشکوک در داده‌های مالی، طبقه‌بندی معاملات به عنوان مشروع یا تقلبی، و ارزیابی ریسک اعتباری استفاده شود.

#### 5.1.2. الگوریتم ماشین بردار پشتیبان (Support Vector Machines - SVM)

الگوریتم **ماشین بردار پشتیبان (Support Vector Machines - SVM)** یکی دیگر از الگوریتم‌های کلاسیک یادگیری ماشین است که در حسابرسی برای طبقه‌بندی داده‌ها استفاده می‌شود. این الگوریتم با یافتن یک **صفحه جداکننده بهینه (optimal separating hyperplane)** بین کلاس‌های مختلف، داده‌ها را طبقه‌بندی می‌کند. SVM به‌ویژه در مواردی که داده‌ها به‌راحتی با یک خط یا صفحه جدا نمی‌شوند (غیرخطی) بسیار مؤثر است، زیرا می‌تواند از توابع هسته (kernel functions) برای تبدیل داده‌ها به یک فضای با بعد بالاتر استفاده کند که در آن داده‌ها قابل جداسازی خطی هستند. در حسابرسی، SVM می‌تواند برای شناسایی تقلب در تراکنش‌های مالی، طبقه‌بندی اسناد، و تشخیص هرزنامه استفاده شود. یکی از مزایای اصلی SVM، **مقاومت آن در برابر overfitting** است، به‌ویژه در فضاهای با بعد بالا. با این حال، انتخاب تابع هسته و تنظیم پارامترهای آن می‌تواند چالش‌برانگیز باشد.

#### 5.1.3. الگوریتم‌های خوشه‌بندی (Clustering Algorithms)

الگوریتم‌های خوشه‌بندی یکی از تکنیک‌های اصلی یادگیری ماشین بدون نظارت هستند که در حسابرسی برای کشف ساختار‌های پنهان در داده‌ها و شناسایی الگوهای غیرمعمول استفاده می‌شوند. این الگوریتم‌ها داده‌ها را به گروه‌هایی (خوشه‌ها) تقسیم می‌کنند که اعضای هر گروه با یکدیگر شباهت بیشتری دارند تا با اعضای گروه‌های دیگر. در یک پژوهش درباره سیستم‌های حسابرسی مالی هوشمند، الگوریتم‌های خوشه‌بندی به‌عنوان یکی از الگوریتم‌های رایج برای کشف تقلب و شناسایی ناهنجاری‌ها معرفی شده‌اند . برای مثال، می‌توان از این الگوریتم‌ها برای خوشه‌بندی معاملات مالی و شناسایی خوشه‌هایی با رفتار غیرمعمول استفاده کرد که ممکن است نشان‌دهنده فعالیت‌های تقلبی باشد. در حسابرسی، این الگوریتم‌ها می‌توانند به‌طور قابل‌توجهی کارایی فرآیند بررسی را افزایش دهند و ریسک‌های پنهان را که ممکن است با روش‌های سنتی قابل شناسایی نباشند، آشکار کنند.

##### 5.1.3.1. K-Means Clustering

الگوریتم **K-Means** یکی از الگوریتم‌های خوشه‌بندی رایج است که داده‌ها را به **K خوشه** با میانگین‌های مشخص تقسیم می‌کند. این الگوریتم با انتخاب K مرکز اولیه شروع می‌کند و سپس به‌طور تکراری داده‌ها را به نزدیک‌ترین مرکز اختصاص می‌دهد و مراکز را بر اساس میانگین داده‌های اختصاص‌یافته به‌روزرسانی می‌کند. K-Means برای داده‌هایی که خوشه‌های کروی شکل دارند بسیار مؤثر است. در حسابرسی، می‌توان از K-Means برای شناسایی گروه‌های مشتریان با رفتارهای مشابه یا خوشه‌بندی معاملات برای کشف تقلب استفاده کرد.

##### 5.1.3.2. DBSCAN Clustering

الگوریتم **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** یک الگوریتم خوشه‌بندی دیگر است که می‌تواند خوشه‌های با شکل دلخواه را شناسایی کند و داده‌های پرت (outliers) را به‌عنوان نویز در نظر بگیرد. برخلاف K-Means، DBSCAN نیازی به تعیین تعداد خوشه‌ها از قبل ندارد. این الگوریتم با شناسایی مناطق با تراکم بالا از داده‌ها، خوشه‌ها را تشکیل می‌دهد. در حسابرسی، DBSCAN می‌تواند برای شناسایی ناهنجاری‌ها در داده‌های مالی یا کشف الگوهای غیرمعمول در رفتار کاربران استفاده شود.

#### 5.1.4. الگوریتم‌های شبکه عصبی (Neural Networks)

الگوریتم‌های شبکه عصبی، به‌ویژه شبکه‌های عصبی مصنوعی (ANN)، یکی از ابزارهای قدرتمند در یادگیری ماشین هستند که در حسابرسی برای حل مسائل پیچیده و غیرخطی استفاده می‌شوند. این الگوریتم‌ها که از ساختار سیستم عصبی بیولوژیکی الهام گرفته‌اند، قادر به یادگیری الگوهای پیچیده در داده‌ها هستند. در یک پژوهش درباره کاربردهای هوش مصنوعی در حسابداری، شبکه‌های عصبی مصنوعی به‌عنوان یکی از سیستم‌های پرقدرت و پرکاربرد برای پیش‌بینی ورشکستگی شرکت‌ها، طبقه‌بندی مشتریان و تشخیص تقلب معرفی شده‌اند . با این حال، این الگوریتم‌ها اغلب به عنوان **"جعبه سیاه"** در نظر گرفته می‌شوند، زیرا تفسیر منطق تصمیم‌گیری آن‌ها دشوار است. این ویژگی می‌تواند چالش‌برانگیز باشد، به‌ویژه در حسابرسی که شفافیت و قابلیت توضیح اهمیت زیادی دارد.

##### 5.1.4.1. شبکه‌های عصبی مصنوعی (Artificial Neural Networks - ANN)

شبکه‌های عصبی مصنوعی (ANN) ساختارهایی هستند که از لایه‌هایی از گره‌های به هم پیوسته (نورون‌ها) تشکیل شده‌اند. هر نورون ورودی‌هایی را دریافت می‌کند، آن‌ها را با وزن‌های مشخصی ترکیب می‌کند، و یک خروجی تولید می‌کند که معمولاً از طریق یک تابع فعال‌سازی (activation function) عبور می‌کند. ANNها می‌توانند برای طبقه‌بندی، رگرسیون و خوشه‌بندی استفاده شوند. در حسابرسی، ANNها می‌توانند برای پیش‌بینی ریسک اعتباری، تشخیص تقلب در معاملات مالی و طبقه‌بندی اسناد استفاده شوند.

##### 5.1.4.2. شبکه‌های عصبی بازگشتی (Recurrent Neural Networks - RNN)

شبکه‌های عصبی بازگشتی (RNN) نوعی از شبکه‌های عصبی هستند که برای پردازش داده‌های دنباله‌ای (sequential data) مانند متن یا سری زمانی طراحی شده‌اند. RNNها دارای حافظه‌ای هستند که اطلاعات از مراحل قبلی را نگه می‌دارد. این ویژگی آن‌ها را برای کاربردهایی مانند تحلیل احساسات متن، پیش‌بینی سری زمانی مالی و تشخیص تقلب در تراکنش‌های دنباله‌ای مناسب می‌کند. در حسابرسی، RNNها می‌توانند برای تحلیل گزارش‌های مالی، پیش‌بینی روندهای بازار و شناسایی الگوهای مشکوک در داده‌های زمانی استفاده شوند.

#### 5.1.5. الگوریتم کاهش بعد (Dimensionality Reduction)

الگوریتم‌های کاهش بعد برای کاهش تعداد ویژگی‌ها (متغیرها) در یک مجموعه داده استفاده می‌شوند، در حالی که اطلاعات مهم حفظ شوند. این تکنیک‌ها در حسابرسی برای تجسم داده‌های با بعد بالا، حذف ویژگی‌های تکراری یا نویزی، و بهبود کارایی الگوریتم‌های یادگیری ماشین بسیار مفید هستند.

##### 5.1.5.1. تحلیل مولفه اصلی (Principal Component Analysis - PCA)

**تحلیل مولفه اصلی (PCA)** یکی از رایج‌ترین تکنیک‌های کاهش بعد است. PCA با یافتن جهت‌هایی (مولفه‌های اصلی) که بیشترین واریانس را در داده‌ها توضیح می‌دهند، عمل می‌کند. سپس داده‌ها را به این مولفه‌های اصلی جدید proyecta می‌کند. در حسابرسی، PCA می‌تواند برای تجسم داده‌های مالی، شناسایی عوامل اصلی که بر ریسک اعتباری تأثیر می‌گذارند، و کاهش پیچیدگی مدل‌های پیش‌بینی استفاده شود.

#### 5.1.6. الگوریتم کشف قوانین انجمن (Association Rule Discovery)

الگوریتم‌های کشف قوانین انجمن برای یافتن روابط جالب بین متغیرها در یک مجموعه داده استفاده می‌شوند. این الگوریتم‌ها در حسابرسی برای شناسایی الگوهای هم‌زمان در داده‌های تراکنشال، مانند تحلیل سبد خرید (market basket analysis)، بسیار مفید هستند.

##### 5.1.6.1. الگوریتم Apriori

الگوریتم **Apriori** یکی از الگوریتم‌های کلاسیک برای کشف قوانین انجمن است. این الگوریتم با استفاده از یک رویکرد تکراری، مجموعه‌هایی از آیتم‌ها (itemsets) را که با فراوانی بالایی ظاهر می‌شوند، شناسایی می‌کند. سپس از این مجموعه‌های آیتم‌ها برای تولید قوانین انجمن استفاده می‌کند. در حسابرسی، Apriori می‌تواند برای شناسایی الگوهای تقلب، مانند یافتن ترکیب‌هایی از محصولات که اغلب در تراکنش‌های تقلبی خریداری می‌شوند، استفاده شود.

### 5.2. الگوریتم‌های حسابرسی سنتی و روش‌های استقرایی (Traditional Auditing & Inductive Methods)

#### 5.2.1. الگوریتم‌های حسابرسی کد (Code Auditing)

الگوریتم‌های حسابرسی کد برای بررسی کد منبع یک سیستم نرم‌افزاری به منظور یافتن آسیب‌پذیری‌های امنیتی، نقض استانداردهای کدنویسی و سایر مشکلات کیفیت استفاده می‌شوند. این الگوریتم‌ها می‌توانند به‌صورت ایستاتیک (بدون اجرای کد) یا پویا (با اجرای کد) عمل کنند. در حسابرسی الگوریتمی، بررسی کد منبع می‌تواند به شناسایی منطق تصمیم‌گیری، ویژگی‌های استفاده‌شده و سایر جزئیات پیاده‌سازی که ممکن است منجر به تبعیض یا نتایج نادرست شوند، کمک کند.

#### 5.2.2. الگوریتم‌های حسابرسی وب‌اسکرپینگ (Scraping Auditing)

الگوریتم‌های حسابرسی وب‌اسکرپینگ برای جمع‌آوری داده‌ها از وب‌سایت‌ها و پلتفرم‌های آنلاین استفاده می‌شوند. این الگوریتم‌ها می‌توانند برای حسابرسی سیستم‌هایی که در فضای وب فعالیت می‌کنند، مانند موتورهای جستجو، سیستم‌های توصیه‌گر و پلتفرم‌های شبکه‌های اجتماعی، استفاده شوند. با جمع‌آوری داده‌هایی مانند نتایج جستجو، توصیه‌ها و محتوای نمایش داده شده، حسابرسان می‌توانند رفتار الگوریتم‌های پشت صحنه را تحلیل کنند.

#### 5.2.3. الگوریتم‌های حسابرسی API (API Auditing)

الگوریتم‌های حسابرسی API برای بررسی و آزمایش APIهای یک سیستم استفاده می‌شوند. این الگوریتم‌ها می‌توانند برای ارسال درخواست‌های مختلف به API، تحلیل پاسخ‌ها و شناسایی رفتارهای غیرمنتظره یا آسیب‌پذیر استفاده شوند. در حسابرسی الگوریتمی، بسیاری از روش‌ها مانند حملات استخراج مدل و حسابرسی خصمانه بر پایه آزمایش APIهای سیستم‌های جعبه سیاه هستند.

#### 5.2.4. الگوریتم‌های حسابرسی مبتنی بر نظرسنجی کاربر (User Survey Auditing)

الگوریتم‌های حسابرسی مبتنی بر نظرسنجی کاربر برای جمع‌آوری بازخورد مستقیم از کاربران یک سیستم استفاده می‌شوند. این روش می‌تواند اطلاعات ارزشمندی درباره تجربه کاربر، کیفیت خروجی‌ها و وجود هرگونه تبعیض یا مشکل ارائه دهد. در حسابرسی الگوریتمی، نظرسنجی‌ها می‌توانند برای بررسی اینکه آیا کاربران مختلف با ویژگی‌های مختلف تجربه‌ای یکسان از سیستم دارند یا خیر، استفاده شوند.