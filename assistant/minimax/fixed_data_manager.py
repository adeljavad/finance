import pandas as pd
import json
import uuid
import logging
import io
from typing import Dict, Any, Optional, List
from django.conf import settings
from .tools.excel_column_mapper import ExcelColumnMapper

logger = logging.getLogger(__name__)

class UserDataManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§
    """
    def __init__(self):
        self.redis_client = None
        self.fallback_storage_dir = "user_data"
        self.user_data_prefix = "user_data:"
        self.user_session_prefix = "user_session:"
        self.column_mapper = ExcelColumnMapper()
        
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Redis
        try:
            import redis
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                db=1,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            # ØªØ³Øª Ø§ØªØµØ§Ù„
            self.redis_client.ping()
            logger.info("âœ… Redis connection successful")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis not available, using file storage: {e}")
            self.redis_client = None
            
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ fallback
        import os
        os.makedirs(self.fallback_storage_dir, exist_ok=True)

    def _get_session_data_key(self, user_id: str) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø±Ø§ÛŒ session"""
        return f"{self.user_session_prefix}{user_id}"

    def _get_dataframe_key(self, user_id: str, df_name: str) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ù„ÛŒØ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø±Ø§ÛŒ DataFrame"""
        return f"{self.user_data_prefix}{user_id}:{df_name}"

    def _save_to_redis(self, key: str, data: str, expire_seconds: int = 3600 * 24 * 7):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Redis Ø¨Ø§ fallback"""
        if self.redis_client:
            try:
                self.redis_client.setex(key, expire_seconds, data)
                return True
            except Exception as e:
                logger.error(f"Redis save error: {e}")
                
        # Fallback to file storage
        try:
            import os
            file_path = os.path.join(self.fallback_storage_dir, f"{key.replace(':', '_')}.json")
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(data)
            logger.info(f"âœ… Saved to file: {file_path}")
            return True
        except Exception as e:
            logger.error(f"File save error: {e}")
            return False

    def _load_from_redis(self, key: str) -> Optional[str]:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² Redis Ø¨Ø§ fallback"""
        if self.redis_client:
            try:
                data = self.redis_client.get(key)
                if data:
                    return data
            except Exception as e:
                logger.error(f"Redis load error: {e}")
                
        # Fallback from file storage
        try:
            import os
            file_path = os.path.join(self.fallback_storage_dir, f"{key.replace(':', '_')}.json")
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = f.read()
                logger.info(f"âœ… Loaded from file: {file_path}")
                return data
        except Exception as e:
            logger.error(f"File load error: {e}")
            
        return None

    def _delete_from_redis(self, key: str) -> bool:
        """Ø­Ø°Ù Ø§Ø² Redis Ø¨Ø§ fallback"""
        deleted = False
        
        if self.redis_client:
            try:
                self.redis_client.delete(key)
                deleted = True
            except Exception as e:
                logger.error(f"Redis delete error: {e}")
                
        # Fallback from file storage
        try:
            import os
            file_path = os.path.join(self.fallback_storage_dir, f"{key.replace(':', '_')}.json")
            if os.path.exists(file_path):
                os.remove(file_path)
                deleted = True
        except Exception as e:
            logger.error(f"File delete error: {e}")
            
        return deleted

    def create_user_session(self, user_id: str = None) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ session Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        if not user_id:
            user_id = str(uuid.uuid4())

        session_data = {
            'user_id': user_id,
            'created_at': pd.Timestamp.now().isoformat(),
            'dataframes': {},
            'uploaded_files': [],
            'analysis_history': []
        }

        session_key = self._get_session_data_key(user_id)
        if self._save_to_redis(session_key, json.dumps(session_data, default=str)):
            logger.info(f"âœ… User session created: {user_id}")
            return user_id
        else:
            logger.error(f"âŒ Failed to create session for user: {user_id}")
            return user_id  # Still return the ID for backward compatibility

    def get_user_session(self, user_id: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª session Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        session_key = self._get_session_data_key(user_id)
        session_data = self._load_from_redis(session_key)
        
        if session_data:
            try:
                return json.loads(session_data)
            except json.JSONDecodeError as e:
                logger.error(f"JSON decode error for user {user_id}: {e}")
                
        # Ø§ÛŒØ¬Ø§Ø¯ session Ø¬Ø¯ÛŒØ¯ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯
        logger.info(f"ğŸ”„ Creating new session for user: {user_id}")
        return self.create_user_session(user_id)

    def save_dataframe(self, user_id: str, df_name: str, dataframe: pd.DataFrame):
        """Ø°Ø®ÛŒØ±Ù‡ DataFrame Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            logger.info(f"ğŸ’¾ Saving DataFrame '{df_name}' for user '{user_id}' - Shape: {dataframe.shape}")
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø§Ù„ÛŒ Ù†Ø¨ÙˆØ¯Ù† DataFrame
            if dataframe.empty:
                logger.warning(f"âš ï¸ DataFrame is empty for user {user_id}: {df_name}")
                return
                
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            cleaned_df = dataframe.copy()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ datetime
            for col in cleaned_df.columns:
                if cleaned_df[col].dtype == 'datetime64[ns]':
                    # ØªØ¨Ø¯ÛŒÙ„ datetime Ø¨Ù‡ string Ø¨Ø±Ø§ÛŒ JSON serialization
                    cleaned_df[col] = cleaned_df[col].astype(str)
                elif cleaned_df[col].dtype == 'object':
                    # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§
                    cleaned_df[col] = cleaned_df[col].fillna('').astype(str)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ JSON Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ù†Ø§Ø³Ø¨
            df_json = cleaned_df.to_json(
                orient='split', 
                date_format='iso',
                force_ascii=False,
                default=str
            )
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Redis/Fallback
            data_key = self._get_dataframe_key(user_id, df_name)
            if self._save_to_redis(data_key, df_json):
                # Ø¢Ù¾Ø¯ÛŒØª session
                session = self.get_user_session(user_id)
                session['dataframes'][df_name] = {
                    'created_at': pd.Timestamp.now().isoformat(),
                    'rows': len(cleaned_df),
                    'columns': list(cleaned_df.columns),
                    'data_types': {col: str(dtype) for col, dtype in cleaned_df.dtypes.items()}
                }
                
                session_key = self._get_session_data_key(user_id)
                self._save_to_redis(session_key, json.dumps(session, default=str))
                logger.info(f"âœ… DataFrame saved successfully for user {user_id}: {df_name} ({len(cleaned_df)} rows)")
            else:
                logger.error(f"âŒ Failed to save DataFrame for user {user_id}: {df_name}")
                
        except Exception as e:
            logger.error(f"âŒ Error saving dataframe for user {user_id}: {e}")
            logger.error(f"DataFrame info - Shape: {dataframe.shape}, Columns: {list(dataframe.columns)}")
            raise

    def get_dataframe(self, user_id: str, df_name: str) -> Optional[pd.DataFrame]:
        """Ø¯Ø±ÛŒØ§ÙØª DataFrame Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            data_key = self._get_dataframe_key(user_id, df_name)
            df_json = self._load_from_redis(data_key)
            
            if df_json:
                # Ø¨Ø§Ø²Ø®ÙˆØ§Ù†ÛŒ JSON Ø¨Ù‡ DataFrame
                dataframe = pd.read_json(df_json, orient='split', dtype=False)
                logger.info(f"âœ… DataFrame loaded successfully for user {user_id}: {df_name} (Shape: {dataframe.shape})")
                return dataframe
            else:
                logger.warning(f"âš ï¸ DataFrame not found for user {user_id}: {df_name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Error loading dataframe for user {user_id}: {e}")
            logger.error(f"Attempted to load key: {self._get_dataframe_key(user_id, df_name)}")
            return None

    def add_uploaded_file(self, user_id: str, file_info: Dict[str, Any]):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡"""
        try:
            session = self.get_user_session(user_id)
            session['uploaded_files'].append({
                **file_info,
                'uploaded_at': pd.Timestamp.now().isoformat()
            })
            session_key = self._get_session_data_key(user_id)
            self._save_to_redis(session_key, json.dumps(session, default=str))
            logger.info(f"âœ… File info added for user {user_id}: {file_info.get('filename', 'unknown')}")
        except Exception as e:
            logger.error(f"âŒ Error adding file info for user {user_id}: {e}")

    def get_user_dataframes_info(self, user_id: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª DataFrameÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
        try:
            session = self.get_user_session(user_id)
            info = session.get('dataframes', {})
            logger.info(f"ğŸ“Š Found {len(info)} DataFrames for user {user_id}")
            return info
        except Exception as e:
            logger.error(f"âŒ Error getting dataframes info for user {user_id}: {e}")
            return {}

    def get_uploaded_files_info(self, user_id: str) -> List[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡"""
        try:
            session = self.get_user_session(user_id)
            files = session.get('uploaded_files', [])
            logger.info(f"ğŸ“ Found {len(files)} uploaded files for user {user_id}")
            return files
        except Exception as e:
            logger.error(f"âŒ Error getting uploaded files info for user {user_id}: {e}")
            return []

    def clear_user_data(self, user_id: str):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
        try:
            logger.info(f"ğŸ—‘ï¸ Clearing all data for user: {user_id}")
            
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† session
            session_key = self._get_session_data_key(user_id)
            self._delete_from_redis(session_key)

            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† DataFrameÙ‡Ø§
            if self.redis_client:
                pattern = f"{self.user_data_prefix}{user_id}:*"
                try:
                    keys = self.redis_client.keys(pattern)
                    if keys:
                        self.redis_client.delete(*keys)
                        logger.info(f"ğŸ—‘ï¸ Deleted {len(keys)} DataFrame keys for user {user_id}")
                except Exception as e:
                    logger.error(f"Redis pattern delete error: {e}")
            else:
                # Fallback file cleanup
                import os
                import glob
                pattern = os.path.join(self.fallback_storage_dir, f"{self.user_data_prefix.replace(':', '_')}{user_id}:*.json")
                files = glob.glob(pattern)
                for file_path in files:
                    try:
                        os.remove(file_path)
                        logger.info(f"ğŸ—‘ï¸ Deleted file: {file_path}")
                    except Exception as e:
                        logger.error(f"File delete error: {e}")

            logger.info(f"âœ… All data cleared for user: {user_id}")
        except Exception as e:
            logger.error(f"âŒ Error clearing user data {user_id}: {e}")

    def debug_user_data(self, user_id: str) -> Dict[str, Any]:
        """Ø¯ÛŒØ¨Ø§Ú¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ troubleshooting"""
        debug_info = {
            'user_id': user_id,
            'has_data': False,
            'dataframes': {},
            'uploaded_files': [],
            'storage_type': 'redis' if self.redis_client else 'file',
            'session_exists': False
        }
        
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ session
            session = self.get_user_session(user_id)
            debug_info['session_exists'] = True
            
            # Ø¨Ø±Ø±Ø³ÛŒ DataFrameÙ‡Ø§
            df_info = self.get_user_dataframes_info(user_id)
            debug_info['dataframes'] = df_info
            debug_info['has_data'] = len(df_info) > 0
            
            # ØªØ³Øª load Ú©Ø±Ø¯Ù† DataFrame Ø§ØµÙ„ÛŒ
            test_df = self.get_dataframe(user_id, 'accounting_data')
            if test_df is not None and not test_df.empty:
                debug_info['accounting_data_status'] = {
                    'loaded': True,
                    'shape': list(test_df.shape),
                    'columns': list(test_df.columns)
                }
                debug_info['has_data'] = True
            else:
                debug_info['accounting_data_status'] = {'loaded': False}
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡
            files_info = self.get_uploaded_files_info(user_id)
            debug_info['uploaded_files'] = files_info
            
            logger.info(f"ğŸ” Debug info for user {user_id}: {debug_info}")
            return debug_info
            
        except Exception as e:
            debug_info['error'] = str(e)
            logger.error(f"âŒ Debug error for user {user_id}: {e}")
            return debug_info

    def process_accounting_file(self, user_id: str, file_content, filename: str) -> pd.DataFrame:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§ÛŒÙ„ Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡"""
        try:
            logger.info(f"ğŸ”„ Processing accounting file: {filename} for user: {user_id}")
            
            # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ ÙØ§ÛŒÙ„
            if filename.lower().endswith('.csv'):
                # Ø¨Ø±Ø§ÛŒ CSV
                if hasattr(file_content, 'read'):
                    dataframe = pd.read_csv(file_content)
                else:
                    dataframe = pd.read_csv(io.StringIO(file_content))
            elif filename.lower().endswith(('.xlsx', '.xls')):
                # Ø¨Ø±Ø§ÛŒ Excel
                if hasattr(file_content, 'read'):
                    dataframe = pd.read_excel(file_content)
                else:
                    dataframe = pd.read_excel(io.BytesIO(file_content))
            else:
                raise ValueError("ÙØ±Ù…Øª ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. ÙÙ‚Ø· CSV Ùˆ Excel Ù…Ø¬Ø§Ø² Ù‡Ø³ØªÙ†Ø¯.")

            # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø§Ù„ÛŒ Ù†Ø¨ÙˆØ¯Ù†
            if dataframe.empty:
                raise ValueError("ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ù†Ø¯Ø§Ø±Ø¯.")

            logger.info(f"ğŸ“Š File loaded - Shape: {dataframe.shape}, Columns: {list(dataframe.columns)}")

            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ (Ø¨Ø§ Ø§Ù†Ø¹Ø·Ø§Ù Ø¨ÛŒØ´ØªØ±)
            required_columns = ['Ø´Ù…Ø§Ø±Ù‡ Ø³Ù†Ø¯', 'ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯', 'Ø¨Ø¯Ù‡Ú©Ø§Ø±', 'Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±', 'ØªÙˆØ¶ÛŒØ­Ø§Øª']
            dataframe_columns_lower = [str(col).strip().lower() for col in dataframe.columns]
            required_columns_lower = [col.lower() for col in required_columns]
            
            missing_columns = []
            for req_col, req_col_lower in zip(required_columns, required_columns_lower):
                if req_col_lower not in dataframe_columns_lower:
                    missing_columns.append(req_col)
                    
            if missing_columns:
                logger.warning(f"âš ï¸ Missing columns: {missing_columns}")
                # Ø¨Ù‡ Ø¬Ø§ÛŒ Ø®Ø·Ø§ØŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ mapping Ú©Ù†ÛŒÙ…
                # (Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ ExcelColumnMapper Ù¾ÛŒØ´Ø±ÙØªÙ‡â€ŒØªØ± Ú©Ø±Ø¯)

            # ØªØ¨Ø¯ÛŒÙ„ Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡
            if 'ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯' in dataframe.columns:
                try:
                    dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'] = pd.to_datetime(
                        dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'],
                        errors='coerce',
                        infer_datetime_format=True
                    )
                    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ string Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
                    dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'] = dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'].astype(str)
                except Exception as e:
                    logger.warning(f"Date conversion warning: {e}")
                    
            if 'Ø¨Ø¯Ù‡Ú©Ø§Ø±' in dataframe.columns:
                dataframe['Ø¨Ø¯Ù‡Ú©Ø§Ø±'] = pd.to_numeric(dataframe['Ø¨Ø¯Ù‡Ú©Ø§Ø±'], errors='coerce').fillna(0)
            if 'Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±' in dataframe.columns:
                dataframe['Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±'] = pd.to_numeric(dataframe['Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±'], errors='coerce').fillna(0)

            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ù„ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            dataframe = dataframe.fillna('')
            
            # Ø°Ø®ÛŒØ±Ù‡ DataFrame
            self.save_dataframe(user_id, 'accounting_data', dataframe)

            # Ø«Ø¨Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„
            self.add_uploaded_file(user_id, {
                'filename': filename,
                'rows': len(dataframe),
                'columns': list(dataframe.columns),
                'total_debit': float(dataframe['Ø¨Ø¯Ù‡Ú©Ø§Ø±'].sum()) if 'Ø¨Ø¯Ù‡Ú©Ø§Ø±' in dataframe.columns else 0,
                'total_credit': float(dataframe['Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±'].sum()) if 'Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±' in dataframe.columns else 0,
                'date_range': {
                    'start': dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'].min() if 'ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯' in dataframe.columns else 'N/A',
                    'end': dataframe['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'].max() if 'ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯' in dataframe.columns else 'N/A'
                }
            })

            logger.info(f"âœ… Accounting file processed successfully for user {user_id}: {filename} ({len(dataframe)} rows)")
            return dataframe
            
        except Exception as e:
            logger.error(f"âŒ Error processing accounting file for user {user_id}: {e}")
            raise

    def get_accounting_summary(self, user_id: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
        try:
            df = self.get_dataframe(user_id, 'accounting_data')
            
            if df is None or df.empty:
                return {
                    'has_data': False,
                    'message': 'Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª',
                    'debug_info': self.debug_user_data(user_id)
                }

            summary = {
                'has_data': True,
                'total_records': len(df),
                'columns': list(df.columns),
                'date_range': {},
                'financial_totals': {},
                'sample_data': df.head(3).to_dict('records') if len(df) > 0 else []
            }

            # Ù…Ø­Ø¯ÙˆØ¯Ù‡ ØªØ§Ø±ÛŒØ®
            if 'ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯' in df.columns:
                try:
                    # ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ datetime ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡
                    date_col = pd.to_datetime(df['ØªØ§Ø±ÛŒØ® Ø³Ù†Ø¯'], errors='coerce')
                    if not date_col.isna().all():
                        summary['date_range'] = {
                            'start': date_col.min().strftime('%Y/%m/%d'),
                            'end': date_col.max().strftime('%Y/%m/%d')
                        }
                except Exception as e:
                    logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ§Ø±ÛŒØ®: {e}")

            # Ù…Ø¬Ù…ÙˆØ¹â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ
            if 'Ø¨Ø¯Ù‡Ú©Ø§Ø±' in df.columns:
                summary['financial_totals']['total_debit'] = float(df['Ø¨Ø¯Ù‡Ú©Ø§Ø±'].sum())
            if 'Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±' in df.columns:
                summary['financial_totals']['total_credit'] = float(df['Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±'].sum())
            if 'Ø¨Ø¯Ù‡Ú©Ø§Ø±' in df.columns and 'Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±' in df.columns:
                summary['financial_totals']['balance'] = float(df['Ø¨Ø¯Ù‡Ú©Ø§Ø±'].sum() - df['Ø¨Ø³ØªØ§Ù†Ú©Ø§Ø±'].sum())

            return summary
            
        except Exception as e:
            logger.error(f"âŒ Error getting accounting summary for user {user_id}: {e}")
            return {
                'has_data': False,
                'error': str(e),
                'debug_info': self.debug_user_data(user_id)
            }

    def _save_mapping_info(self, user_id: str, mapping_result: Dict, filename: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù¾ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø¹Ø¯ÛŒ"""
        try:
            mapping_key = f"{self.user_data_prefix}{user_id}:mapping_info"
            mapping_data = {
                'filename': filename,
                'timestamp': pd.Timestamp.now().isoformat(),
                'mapping_result': mapping_result
            }
            
            # Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù„ÛŒØ³Øª Ù…ÙˆØ¬ÙˆØ¯
            existing_mappings_json = self._load_from_redis(mapping_key)
            if existing_mappings_json:
                mappings_list = json.loads(existing_mappings_json)
            else:
                mappings_list = []
                
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù¾ÛŒÙ†Ú¯ Ø¬Ø¯ÛŒØ¯
            mappings_list.append(mapping_data)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡
            self._save_to_redis(mapping_key, json.dumps(mappings_list, default=str))
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù¾ÛŒÙ†Ú¯: {e}")

    def get_mapping_history(self, user_id: str) -> List[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ù¾ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
        try:
            mapping_key = f"{self.user_data_prefix}{user_id}:mapping_info"
            mappings_data = self._load_from_redis(mapping_key)
            
            if mappings_data:
                return json.loads(mappings_data)
            else:
                return []
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ù¾ÛŒÙ†Ú¯: {e}")
            return []